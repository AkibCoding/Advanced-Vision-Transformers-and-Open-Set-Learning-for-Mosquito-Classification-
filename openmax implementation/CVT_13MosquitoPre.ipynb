{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4b2Rf-wzL16",
        "outputId": "06689b68-6b50-46f9-c537-51f82dab43b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision timm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qg8YmQxu0alh",
        "outputId": "01d5961a-fb1b-4370-b735-3c170622f0ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.1+cu121)\n",
            "Collecting timm\n",
            "  Downloading timm-0.9.16-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.20.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.2.2)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, timm\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 timm-0.9.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from transformers import AutoFeatureExtractor, CvtForImageClassification\n",
        "from PIL import Image\n",
        "import requests\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Data preprocessing\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Dataset and DataLoader setup\n",
        "train_dataset = datasets.ImageFolder('/content/drive/MyDrive/datasets/trainset', transform=train_transform)\n",
        "test_dataset = datasets.ImageFolder('/content/drive/MyDrive/datasets/testset', transform=test_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Model setup using CVT-13\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained('microsoft/cvt-13', do_rescale=False)\n",
        "model = CvtForImageClassification.from_pretrained('microsoft/cvt-13', num_labels=len(train_dataset.classes), ignore_mismatched_sizes=True).to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Training and evaluation\n",
        "num_epochs = 10\n",
        "train_losses = []\n",
        "val_accuracies = []\n",
        "val_f1_scores = []\n",
        "val_precisions = []\n",
        "val_recalls = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        inputs = feature_extractor(images=images, return_tensors=\"pt\").to(device)\n",
        "        outputs = model(**inputs)\n",
        "        loss = criterion(outputs.logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.logits.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_accuracy = 100 * correct / total\n",
        "    train_losses.append(train_loss / len(train_loader))\n",
        "\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            inputs = feature_extractor(images=images, return_tensors=\"pt\").to(device)\n",
        "            outputs = model(**inputs)\n",
        "            _, preds = torch.max(outputs.logits, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_accuracy = 100 * accuracy_score(all_labels, all_preds)\n",
        "    f1 = 100 * f1_score(all_labels, all_preds, average='macro')\n",
        "    precision = 100 * precision_score(all_labels, all_preds, average='macro')\n",
        "    recall = 100 * recall_score(all_labels, all_preds, average='macro')\n",
        "\n",
        "    val_accuracies.append(val_accuracy)\n",
        "    val_f1_scores.append(f1)\n",
        "    val_precisions.append(precision)\n",
        "    val_recalls.append(recall)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
        "          f'Train Loss: {train_loss / len(train_loader):.4f}, '\n",
        "          f'Train Accuracy: {train_accuracy:.2f}%, '\n",
        "          f'Val Accuracy: {val_accuracy:.2f}%, '\n",
        "          f'F1 Score: {f1:.2f}%, '\n",
        "          f'Precision: {precision:.2f}%, '\n",
        "          f'Recall: {recall:.2f}%')\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/cvt_13_model.pth')\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "# Plot Training Loss and Validation Accuracy\n",
        "ax1 = plt.subplot(1, 2, 1)\n",
        "lns1 = ax1.plot(range(1, num_epochs + 1), train_losses, label='Training Loss', color='blue', linestyle='-')\n",
        "ax2 = ax1.twinx()\n",
        "lns2 = ax2.plot(range(1, num_epochs + 1), val_accuracies, label='Validation Accuracy', color='green', linestyle='--')\n",
        "\n",
        "lns = lns1 + lns2\n",
        "labs = [l.get_label() for l in lns]\n",
        "ax1.legend(lns, labs, loc='upper left')\n",
        "\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Training Loss', color='blue')\n",
        "ax2.set_ylabel('Validation Accuracy', color='green')\n",
        "ax1.tick_params(axis='y', labelcolor='blue')\n",
        "ax2.tick_params(axis='y', labelcolor='green')\n",
        "\n",
        "plt.title('Training Loss & Validation Accuracy')\n",
        "\n",
        "# Plot Validation Metrics\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, num_epochs + 1), val_f1_scores, label='F1 Score', linestyle='--', color='red')\n",
        "plt.plot(range(1, num_epochs + 1), val_precisions, label='Precision', linestyle='-.', color='purple')\n",
        "plt.plot(range(1, num_epochs + 1), val_recalls, label='Recall', linestyle=':', color='orange')\n",
        "plt.title('Validation Metrics')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Percentage')\n",
        "plt.legend(loc='best')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpGb5PQV14CL",
        "outputId": "058f0a71-da07-4748-fb0d-edc3d7f7478f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of CvtForImageClassification were not initialized from the model checkpoint at microsoft/cvt-13 and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([1000, 384]) in the checkpoint and torch.Size([10, 384]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Train Loss: 1.2796, Train Accuracy: 56.00%, Val Accuracy: 70.40%, F1 Score: 66.79%, Precision: 79.07%, Recall: 70.40%\n",
            "Epoch [2/10], Train Loss: 0.7807, Train Accuracy: 71.33%, Val Accuracy: 90.90%, F1 Score: 90.97%, Precision: 91.60%, Recall: 90.90%\n",
            "Epoch [3/10], Train Loss: 0.6473, Train Accuracy: 75.83%, Val Accuracy: 93.60%, F1 Score: 93.61%, Precision: 93.80%, Recall: 93.60%\n",
            "Epoch [4/10], Train Loss: 0.6043, Train Accuracy: 77.38%, Val Accuracy: 84.60%, F1 Score: 81.54%, Precision: 90.11%, Recall: 84.60%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install libmr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxwZh3bfyFnp",
        "outputId": "3dea287a-f8c4-4a16-c9ef-dce3a953a534"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting libmr\n",
            "  Downloading libmr-0.1.9.zip (39 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from libmr) (3.0.10)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from libmr) (1.25.2)\n",
            "Building wheels for collected packages: libmr\n",
            "  Building wheel for libmr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libmr: filename=libmr-0.1.9-cp310-cp310-linux_x86_64.whl size=549302 sha256=73698cba56c0198676d3199ea8dd61e38896db04e93324203fb3944b9bbb2ec2\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/e0/45/ec5f4f802b034150d6f5735922408cc0278bf85582b2a1a954\n",
            "Successfully built libmr\n",
            "Installing collected packages: libmr\n",
            "Successfully installed libmr-0.1.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoFeatureExtractor, CvtForImageClassification, CvtConfig\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# Define device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Custom model class to access specific layer outputs\n",
        "class CustomCvtForImageClassification(CvtForImageClassification):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "    def forward(self, pixel_values, output_hidden_states=False):\n",
        "        outputs = super().forward(pixel_values, output_hidden_states=True)\n",
        "        if output_hidden_states:\n",
        "            return outputs.hidden_states[-2]  # Adjust based on the model architecture\n",
        "        return outputs.logits\n",
        "\n",
        "# Initialize model\n",
        "config = CvtConfig.from_pretrained('microsoft/cvt-13', num_labels=10)\n",
        "model = CustomCvtForImageClassification(config)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/mosquit_model7(CVT-13)/cvt_13_model.pth', map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Data loaders\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "train_dataset = datasets.ImageFolder('/content/drive/MyDrive/datasets/trainset', transform=train_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "test_dataset = datasets.ImageFolder('/content/drive/MyDrive/TestSet_Openmax', transform=test_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# Feature extractor setup, assuming no rescale is needed as images are already in [0,1]\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained('microsoft/cvt-13', do_rescale=False)\n",
        "\n",
        "# Get activations function\n",
        "def get_activations(dataloader, model):\n",
        "    model.eval()\n",
        "    activations = []\n",
        "    labels = []\n",
        "    for images, labels_batch in dataloader:\n",
        "        images = images.to(device)\n",
        "        with torch.no_grad():\n",
        "            activation = model(images, output_hidden_states=True)\n",
        "        activations.append(activation.cpu().numpy())\n",
        "        labels.extend(labels_batch.cpu().numpy())\n",
        "    return np.array(activations), np.array(labels)\n",
        "\n",
        "activations, labels = get_activations(train_loader, model)\n",
        "\n",
        "# Dummy functions for MAVs and Weibull fitting (implement these based on your specifics)\n",
        "def compute_mavs(activations, labels):\n",
        "    return np.mean(activations, axis=0)\n",
        "\n",
        "def fit_weibull(mavs, activations):\n",
        "    return {}\n",
        "\n",
        "mavs = compute_mavs(activations, labels)\n",
        "weibull_models = fit_weibull(mavs, activations)\n",
        "\n",
        "# OpenMax score application function (implement the real logic)\n",
        "def apply_openmax(scores, mavs, weibull_models):\n",
        "    return scores\n",
        "\n",
        "# OpenMax inference\n",
        "def openmax_inference(test_loader, model, feature_extractor, mavs, weibull_models, threshold=0.5):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    for images, labels_batch in test_loader:\n",
        "        images = images.to(device)\n",
        "        inputs = feature_extractor(images=images, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            scores = softmax(outputs.cpu().numpy(), axis=1)\n",
        "            openmax_scores = apply_openmax(scores, mavs, weibull_models)\n",
        "        pred = np.argmax(openmax_scores, axis=1)\n",
        "        pred = np.where(np.max(openmax_scores, axis=1) < threshold, len(train_dataset.classes), pred)\n",
        "        all_preds.extend(pred)\n",
        "        all_labels.extend(labels_batch.numpy())\n",
        "    return accuracy_score(all_labels, all_preds)\n",
        "\n",
        "test_accuracy = openmax_inference(test_loader, model, feature_extractor, mavs, weibull_models)\n",
        "print(\"Test accuracy with OpenMax:\", test_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Imu6jp8hGnof",
        "outputId": "818b0e3e-2335-4ffb-821a-4bff94ebc31e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/convnext/feature_extraction_convnext.py:28: FutureWarning: The class ConvNextFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ConvNextImageProcessor instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy with OpenMax: 0.6972477064220184\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoFeatureExtractor, CvtForImageClassification, CvtConfig\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# Define device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Custom model class to access specific layer outputs\n",
        "class CustomCvtForImageClassification(CvtForImageClassification):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "    def forward(self, pixel_values, output_hidden_states=False):\n",
        "        outputs = super().forward(pixel_values, output_hidden_states=True)\n",
        "        if output_hidden_states:\n",
        "            return outputs.hidden_states[-2]  # Adjust based on the model architecture\n",
        "        return outputs.logits\n",
        "\n",
        "# Initialize model\n",
        "config = CvtConfig.from_pretrained('microsoft/cvt-13', num_labels=10)\n",
        "model = CustomCvtForImageClassification(config)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/mosquit_model7(CVT-13)/cvt_13_model.pth', map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Data loaders\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "train_dataset = datasets.ImageFolder('/content/drive/MyDrive/datasets/trainset', transform=train_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "test_dataset = datasets.ImageFolder('/content/drive/MyDrive/TestSet_Openmax', transform=test_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# Feature extractor setup, assuming no rescale is needed as images are already in [0,1]\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained('microsoft/cvt-13', do_rescale=False)\n",
        "\n",
        "# Get activations function\n",
        "def get_activations(dataloader, model):\n",
        "    model.eval()\n",
        "    activations = []\n",
        "    labels = []\n",
        "    for images, labels_batch in dataloader:\n",
        "        images = images.to(device)\n",
        "        with torch.no_grad():\n",
        "            activation = model(images, output_hidden_states=True)\n",
        "        activations.append(activation.cpu().numpy())\n",
        "        labels.extend(labels_batch.cpu().numpy())\n",
        "    return np.array(activations), np.array(labels)\n",
        "\n",
        "activations, labels = get_activations(train_loader, model)\n",
        "\n",
        "# Dummy functions for MAVs and Weibull fitting (implement these based on your specifics)\n",
        "def compute_mavs(activations, labels):\n",
        "    return np.mean(activations, axis=0)\n",
        "\n",
        "def fit_weibull(mavs, activations):\n",
        "    return {}\n",
        "\n",
        "mavs = compute_mavs(activations, labels)\n",
        "weibull_models = fit_weibull(mavs, activations)\n",
        "\n",
        "# OpenMax score application function\n",
        "def apply_openmax(scores, mavs, weibull_models):\n",
        "    openmax_scores = []\n",
        "    for score in scores:\n",
        "        openmax_score = np.zeros(len(mavs) + 1)  # Add one for the unknown class\n",
        "        # Reshape score to match the last dimensions of mavs\n",
        "        score = np.expand_dims(score, axis=0)  # Add a new axis\n",
        "        score = np.expand_dims(score, axis=0)  # Add two new axes\n",
        "        score = np.expand_dims(score, axis=-1)  # Add one new axis\n",
        "        distances = np.linalg.norm(mavs - score, axis=(1, 2, 3))\n",
        "        for i in range(len(mavs)):\n",
        "            w_score = weibull_models[i].w_score(distances[i])\n",
        "            openmax_score[i] = score[i] * (1 - w_score)\n",
        "        openmax_score[-1] = np.sum(score * [weibull_models[i].w_score(distances[i]) for i in range(len(mavs))])\n",
        "        openmax_scores.append(openmax_score)\n",
        "    return np.array(openmax_scores)\n",
        "\n",
        "\n",
        "\n",
        "# OpenMax inference\n",
        "def openmax_inference(test_loader, model, feature_extractor, mavs, weibull_models, threshold=0.5):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    for images, labels_batch in test_loader:\n",
        "        images = images.to(device)\n",
        "        inputs = feature_extractor(images=images, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            scores = softmax(outputs.cpu().numpy(), axis=1)\n",
        "            openmax_scores = apply_openmax(scores, mavs, weibull_models)\n",
        "        pred = np.argmax(openmax_scores, axis=1)\n",
        "        pred = np.where(np.max(openmax_scores, axis=1) < threshold, len(train_dataset.classes), pred)\n",
        "        all_preds.extend(pred)\n",
        "        all_labels.extend(labels_batch.numpy())\n",
        "    return accuracy_score(all_labels, all_preds)\n",
        "\n",
        "test_accuracy = openmax_inference(test_loader, model, feature_extractor, mavs, weibull_models)\n",
        "print(\"Test accuracy with OpenMax:\", test_accuracy)\n"
      ],
      "metadata": {
        "id": "AnLO5Kg34o83"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}